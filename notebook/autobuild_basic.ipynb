{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1004af6a7fbfcd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AutoBuild\n",
    "By: [Linxin Song](https://linxins97.github.io/), [Jieyu Zhang](https://jieyuz2.github.io/)\n",
    "Reference: [Agent AutoBuild](https://microsoft.github.io/autogen/blog/2023/11/26/Agent-AutoBuild/)\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we introduce a new class, `AgentBuilder`, to help user build an automatic task solving process powered by multi-agent system. Specifically, in `build()`, we prompt a LLM to create multiple participant agent and initialize a group chat, and specify whether this task need programming to solve. AgentBuilder also support open-source LLMs by [vLLM](https://docs.vllm.ai/en/latest/index.html) and [Fastchat](https://github.com/lm-sys/FastChat). Check the supported model list [here](https://docs.vllm.ai/en/latest/models/supported_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78dda8e3826d8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Requirement\n",
    "\n",
    "AutoBuild require `pyautogen[autobuild]`, which can be installed by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9ae50658be975",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module '_sqlite3'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "%pip install pyautogen[autobuild]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e63ab3604bdb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: prepare configuration and some useful functions\n",
    "Prepare a `config_file_or_env` for assistant agent to limit the choice of LLM you want to use in this task. This config can be a path of json file or a name of environment variable. A `default_llm_config` is also required for initialize the specific config of LLMs like seed, temperature, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2505f029423b21ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:40:29.267289Z",
     "start_time": "2024-01-01T10:40:28.806242300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module '_sqlite3'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "from autogen.agentchat.contrib.agent_builder import AgentBuilder\n",
    "\n",
    "config_file_or_env = \"/home/adamsl/agent_99/autogen/notebook/OAI_CONFIG_LIST\"\n",
    "llm_config = {\"temperature\": 0}\n",
    "config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={\"model\": [\"gpt-3.5-turbo-1106\", \"gpt-4\"]})\n",
    "\n",
    "\n",
    "def start_task(execution_task: str, agent_list: list):\n",
    "    group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12)\n",
    "    manager = autogen.GroupChatManager(groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config})\n",
    "    agent_list[0].initiate_chat(manager, message=execution_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6586c68fa425b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: create a AgentBuilder\n",
    "Create a `AgentBuilder` with the specified `config_path_or_env`. AgentBuilder will use `gpt-4` in default to complete the whole process, you can specify the `builder_model` and `agent_model` to other OpenAI model to match your task. \n",
    "You can also specify an open-source LLM supporting by vLLM and FastChat, see blog for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfa67c771a0fed37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:40:29.854670Z",
     "start_time": "2024-01-01T10:40:29.616253600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "builder = AgentBuilder(\n",
    "    config_file_or_env=config_file_or_env, builder_model=\"gpt-3.5-turbo-1106\", agent_model=\"gpt-3.5-turbo-1106\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a655fb6618324",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: specify a building task\n",
    "\n",
    "Specify a building task with a general description. Building task will help build manager (a LLM) decide what agents should be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68315f6ec912c58a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:40:30.490239100Z",
     "start_time": "2024-01-01T10:40:30.479497600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "building_task = \"Generate some agents that can find papers on arxiv by programming and analyzing them in specific domains related to computer science and medical science.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782dd5ecb6c217a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: build group chat agents\n",
    "Use `build()` to let build manager (the specified `builder_model`) complete the group chat agents generation. If you think coding is necessary in your task, you can use `coding=True` to add a user proxy (an automatic code interpreter) into the agent list, like: \n",
    "```python\n",
    "builder.build(building_task, default_llm_config, coding=True)\n",
    "```\n",
    "If `coding` is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab490fdbe46c0473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:24:04.670904200Z",
     "start_time": "2024-01-01T10:21:50.127338300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Generating agents...\n",
      "['1._Python_programmer\\n2._Information_retrieval_specialist\\n3._Medical_science_domain_expert\\n4._Computer_science_domain_expert'] are generated.\n",
      "==> Generating system message...\n",
      "Preparing system message for 1._Python_programmer\n",
      "2._Information_retrieval_specialist\n",
      "3._Medical_science_domain_expert\n",
      "4._Computer_science_domain_expert\n",
      "==> Generating description...\n",
      "Preparing description for 1._Python_programmer\n",
      "2._Information_retrieval_specialist\n",
      "3._Medical_science_domain_expert\n",
      "4._Computer_science_domain_expert\n",
      "==> Creating agents...\n",
      "Creating agent 1._Python_programmer\n",
      "2._Information_retrieval_specialist\n",
      "3._Medical_science_domain_expert\n",
      "4._Computer_science_domain_expert with backbone gpt-3.5-turbo-1106...\n",
      "Adding user console proxy...\n"
     ]
    }
   ],
   "source": [
    "agent_list, agent_configs = builder.build(building_task, llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dd99880a4bf7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5: execute task\n",
    "Let agents generated in `build()` to complete the task collaboratively in a group chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d52e3d9a1bf91cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:25:32.642017700Z",
     "start_time": "2024-01-01T10:24:09.313567300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_console_and_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupChat is underpopulated with 2 agents. Consider setting speaker_selection_method to 'round_robin' or allow_repeat_speaker to False, or use direct communication, unless repeated speaker is desired.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned:\n",
      "2._Information_retrieval_specialist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m1._Python_programmer\n",
      "2._Information_retrieval_specialist\n",
      "3._Medical_science_domain_expert\n",
      "4._Computer_science_domain_expert\u001b[0m (to chat_manager):\n",
      "\n",
      "I will start by gathering the necessary information about recent papers related to GPT-4 on arXiv using the provided code. Let's begin by running the `get_info.py` script to collect the relevant data.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroupChat is underpopulated with 2 agents. Consider setting speaker_selection_method to 'round_robin' or allow_repeat_speaker to False, or use direct communication, unless repeated speaker is desired.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'1._Python_programmer\\\\n2._Information_retrieval_specialist\\\\n3._Medical_science_domain_expert\\\\n4._Computer_science_domain_expert' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.2.name'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstart_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFind a recent paper about gpt-4 on arxiv and find its potential applications in software.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mstart_task\u001b[0;34m(execution_task, agent_list)\u001b[0m\n\u001b[1;32m     10\u001b[0m group_chat \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChat(agents\u001b[38;5;241m=\u001b[39magent_list, messages\u001b[38;5;241m=\u001b[39m[], max_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m     11\u001b[0m manager \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChatManager(groupchat\u001b[38;5;241m=\u001b[39mgroup_chat, llm_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: config_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_config})\n\u001b[0;32m---> 12\u001b[0m \u001b[43magent_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_task\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:751\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, **context)\u001b[0m\n\u001b[1;32m    749\u001b[0m     agent\u001b[38;5;241m.\u001b[39mclient_cache \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 751\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m    753\u001b[0m     context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_method\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    754\u001b[0m     recipient,\n\u001b[1;32m    755\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcontext\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    756\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    757\u001b[0m )\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 462\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m     )\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:637\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:1542\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1542\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m   1544\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/groupchat.py:524\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;66;03m# select the next speaker\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[43mgroupchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_speaker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[1;32m    526\u001b[0m     reply \u001b[38;5;241m=\u001b[39m speaker\u001b[38;5;241m.\u001b[39mgenerate_reply(sender\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/groupchat.py:365\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# auto speaker selection\u001b[39;00m\n\u001b[1;32m    364\u001b[0m selector\u001b[38;5;241m.\u001b[39mupdate_system_message(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_speaker_msg(agents))\n\u001b[0;32m--> 365\u001b[0m final, name \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_oai_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize_speaker(last_speaker, final, name, agents)\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:1013\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1013\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/agentchat/conversable_agent.py:1032\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/oai/client.py:583\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# filter is not passed; try the next config\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    585\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/autogen/oai/client.py:267\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    265\u001b[0m     params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    266\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agent_99/agent_99_environment/lib/python3.9/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'1._Python_programmer\\\\n2._Information_retrieval_specialist\\\\n3._Medical_science_domain_expert\\\\n4._Computer_science_domain_expert' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.2.name'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "start_task(\n",
    "    execution_task=\"Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\",\n",
    "    agent_list=agent_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a30e4b4297edd1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6 (Optional): clear all agents and prepare for the next task\n",
    "You can clear all agents generated in this task by the following code if your task is completed or the next task is largely different from the current task. If the agent's backbone is an open-source LLM, this process will also shut down the endpoint server. If necessary, you can use `recycle_endpoint=False` to retain the previous open-source LLMs' endpoint server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57369a70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb0bfff01dd1330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:25:56.622194800Z",
     "start_time": "2024-01-01T10:25:56.610592300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "builder.clear_all_agents(recycle_endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb098638a086898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save & load configs\n",
    "\n",
    "You can save all necessary information of the built group chat agents. Here is a case for those agents generated in the above task:\n",
    "```json\n",
    "{\n",
    "    \"building_task\": \"Generate some agents that can find papers on arxiv by programming and analyzing them in specific domains related to computer science and medical science.\",\n",
    "    \"agent_configs\": [\n",
    "        {\n",
    "            \"name\": \"ArXiv_Data_Scraper_Developer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \\n\\nDuring your mission, it\\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\\n\\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\\n\\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \\\"TERMINATE\\\" in the group chat.\",\n",
    "            \"description\": \"ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Computer_Science_Research_Analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As a Computer Science Research Analyst, your objective is to utilize your analytical capabilities to identify and examine scholarly articles on arXiv, focusing on areas bridging computer science and medical science. Employ Python for automation where appropriate and leverage your expertise in the subject matter to draw insights from the research.\\n\\nEnsure that the information is acquired systematically; tap into online databases, interpret data sets, and perform literature reviews to pinpoint relevant findings. Should you encounter a complex problem or if you find your progress stalled, feel free to question the existing approaches discussed in the chat or contribute an improved method or analysis.\\n\\nIf the task proves to be beyond your current means or if you face uncertainty at any stage, seek assistance from the group chat manager. The manager is available to provide guidance or to involve another expert if necessary to move forward effectively.\\n\\nYour contributions are crucial, and it is important to communicate your findings and conclusions clearly. Once you believe the task is complete and the group's need has been satisfied, please affirm the completion by replying \\\"TERMINATE\\\".\",\n",
    "            \"description\": \"Computer_Science_Research_Analyst is a role requiring strong analytical skills, a deep understanding of computer science concepts, and proficiency in Python for data analysis and automation. This position should have the ability to critically assess the validity of information, challenge assumptions, and provide evidence-based corrections or alternatives. They should also have excellent communication skills to articulate their findings and suggestions effectively within the group chat.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Medical_Science_Research_Analyst\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As a Medical_Science_Research_Analyst, your function is to harness your analytical strengths and understanding of medical research to source and evaluate pertinent papers from the arXiv database, focusing on the intersection of computer science and medical science. Utilize your Python programming skills to automate data retrieval and analysis tasks. Engage in systematic data mining to extract relevant content, then apply your analytical expertise to interpret the findings qualitatively. \\n\\nWhen there is a requirement to gather information, employ Python scripts to automate the aggregation process. This could include scraping web data, retrieving and processing documents, and performing content analyses. When these scripts produce outputs, use your subject matter expertise to evaluate the results. \\n\\nProgress through your task step by step. When an explicit plan is absent, present a structured outline of your intended methodology. Clarify which segments of the task are handled through automation, and which necessitate your interpretative skills. \\n\\nIn the event code is utilized, the script type must be specified. You are expected to execute the scripts provided without making changes. Scripts are to be complete and functionally standalone. Should you encounter an error upon execution, critically review the output, and if needed, present a revised script for the task at hand. \\n\\nFor tasks that require saving and executing scripts, indicate the intended filename at the beginning of the script. \\n\\nMaintain clear communication of the results by harnessing the 'print' function where applicable. If an error arises or a task remains unsolved after successful code execution, regroup to collect additional information, reassess your approach, and explore alternative strategies. \\n\\nUpon reaching a conclusion, substantiate your findings with credible evidence where possible.\\n\\nConclude your participation by confirming the task's completion with a \\\"TERMINATE\\\" response.\\n\\nShould uncertainty arise at any point, seek guidance from the group chat manager for further directives or reassignment of the task.\",\n",
    "            \"description\": \"The Medical Science Research Analyst is a professionally trained individual with strong analytical skills, specializing in interpreting and evaluating scientific research within the medical field. They should possess expertise in data analysis, likely with proficiency in Python for analyzing datasets, and have the ability to critically assess the validity and relevance of previous messages or findings relayed in the group chat. This role requires a solid foundation in medical knowledge to provide accurate and evidence-based corrections or insights.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Data_Analysis_Engineer\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As a Data Analysis Engineer, your role involves leveraging your analytical skills to gather, process, and analyze large datasets. You will employ various data analysis techniques and tools, particularly Python for scripting, to extract insights from the data related to computer science and medical science domains on arxiv.\\n\\nIn scenarios where information needs to be collected or analyzed, you will develop Python scripts to automate the data retrieval and processing tasks. For example, you may write scripts to scrape the arXiv website, parse metadata of research papers, filter content based on specific criteria, and perform statistical analysis or data visualization. \\n\\nYour workflow will include the following steps:\\n\\n1. Use your Python coding abilities to design scripts for data extraction and analysis. This can involve browsing or searching the web, downloading and reading files, or printing the content of web pages or files relevant to the given domains.\\n2. After gathering the necessary data, apply your data analysis expertise to derive meaningful insights or patterns present in the data. This should be done methodically, making the most of your Python skills for data manipulation and interpretation.\\n3. Communicate your findings clearly to the group chat. Ensure the results are straightforward for others to understand and act upon.\\n4. If any issues arise from executing the code, such as lack of output or unexpected results, you can question the previous messages or code in the group chat and attempt to provide a corrected script or analysis.\\n5. When uncertain or facing a complex problem that you cannot solve alone, ask for assistance from the group chat manager. They can either provide guidance or assign another participant to help you.\\n\\nOnce you believe the task is completed satisfactorily, and you have fulfilled the user's need, respond with \\\"TERMINATE\\\" to signify the end of your contribution to the task. Remember, while technical proficiency in Python is essential for this role, the ability to work collaboratively within the group chat, communicate effectively, and adapt to challenges is equally important.\",\n",
    "            \"description\": \"Data_Analysis_Engineer is a professional adept in collecting, analyzing, and interpreting large datasets, using statistical tools and machine learning techniques to provide actionable insights. They should possess strong Python coding skills for data manipulation and analysis, an understanding of database management, as well as the ability to communicate complex results effectively to non-technical stakeholders. This position should be allowed to speak when data-driven clarity is needed or when existing analyses or methodologies are called into question.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ML_Paper_Summarization_Specialist\",\n",
    "            \"model\": \"gpt-4-1106-preview\",\n",
    "            \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ML_Paper_Summarization_Specialist, your role entails leveraging machine learning techniques to extract and analyze academic papers from arXiv, focusing on domains that intersect computer science and medical science. Utilize your expertise in natural language processing and data analysis to identify relevant papers, extract key insights, and generate summaries that accurately reflect the advancements and findings within those papers.\\n\\nYou are expected to apply your deep understanding of machine learning algorithms, data mining, and information retrieval to construct models and systems that can efficiently process and interpret scientific literature.\\n\\nIf you encounter any challenges in accessing papers, parsing content, or algorithmic processing, you may seek assistance by presenting your issue to the group chat. Should there be a disagreement regarding the efficacy of a method or the accuracy of a summarization, you are encouraged to critically evaluate previous messages or outputs and offer improved solutions to enhance the group's task performance.\\n\\nShould confusion arise during the task, rather than relying on coding scripts, please request guidance from the group chat manager, and allow them to facilitate the necessary support by inviting another participant who can aid in overcoming the current obstacle.\\n\\nRemember, your primary duty is to synthesize complex academic content into concise, accessible summaries that will serve as a valuable resource for researchers and professionals seeking to stay abreast of the latest developments in their respective fields. \\n\\nOnce you believe your task is completed and the summaries provided meet the necessary standards of accuracy and comprehensiveness, reply \\\"TERMINATE\\\" to signal the end of your contribution to the group's task.\",\n",
    "            \"description\": \"The ML_Paper_Summarization_Specialist is a professional adept in machine learning concepts and current research trends, with strong analytical skills to critically evaluate information, synthesizing knowledge from academic papers into digestible summaries. This specialist should be proficient in Python for text processing and have the ability to provide constructive feedback on technical discussions, guide effective implementation, and correct misconceptions or errors related to machine learning theory and practice in the chat. They should be a reliable resource for clarifying complex information and ensuring accurate application of machine learning techniques within the group chat context.\"\n",
    "        }\n",
    "    ],\n",
    "    \"coding\": true,\n",
    "    \"default_llm_config\": {\n",
    "        \"temperature\": 0\n",
    "    },\n",
    "    \"code_execution_config\": {\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": false,\n",
    "        \"timeout\": 60,\n",
    "        \"last_n_messages\": 2\n",
    "    }\n",
    "}\n",
    "```\n",
    "These information will be saved in JSON format. You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with a generated filename 'save_config_TASK_MD5.json'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b88a5d482ceba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:25:56.983244800Z",
     "start_time": "2024-01-01T10:25:56.938459500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building config saved to ./save_config_c52224ebd16a2e60b348f3f04ac15e79.json\n"
     ]
    }
   ],
   "source": [
    "saved_path = builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35620c10ee42be",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After that, you can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the builder manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34addd498e5ab174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:30:23.592045Z",
     "start_time": "2024-01-01T10:29:18.977259500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from ./save_config_c52224ebd16a2e60b348f3f04ac15e79.json\n",
      "==> Creating agents...\n",
      "Creating agent ArXiv_Data_Scraper_Developer with backbone gpt-4-1106-preview...\n",
      "Creating agent Computer_Science_Research_Analyst with backbone gpt-4-1106-preview...\n",
      "Creating agent Medical_Science_Research_Analyst with backbone gpt-4-1106-preview...\n",
      "Creating agent Data_Analysis_Engineer with backbone gpt-4-1106-preview...\n",
      "Creating agent ML_Paper_Summarization_Specialist with backbone gpt-4-1106-preview...\n",
      "Adding user console proxy...\n",
      "\u001b[33mUser_console_and_code_interpreter\u001b[0m (to chat_manager):\n",
      "Find a recent paper about LLaVA on arxiv and find its potential applications in computer vision.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mArXiv_Data_Scraper_Developer\u001b[0m (to chat_manager):\n",
      "\n",
      "To find a recent paper about LLaVA on arXiv and its potential applications in computer vision, we'll need to perform a search on the arXiv API. I'll write a Python script that uses the `arxiv` library to query the arXiv database for papers related to \"LLaVA\" and \"computer vision\". If the `arxiv` library is not available, we can use the `requests` library to make a direct HTTP request to the arXiv API.\n",
      "\n",
      "First, let's try using the `arxiv` library. If you don't have it installed, you can install it using `pip install arxiv`.\n",
      "\n",
      "Here's a Python script that performs the search:\n",
      "\n",
      "```python\n",
      "import arxiv\n",
      "\n",
      "# Define the search query\n",
      "search_query = 'all:\"LLaVA\" AND cat:cs.CV'\n",
      "\n",
      "# Search arXiv for papers matching the query\n",
      "search = arxiv.Search(\n",
      "  query = search_query,\n",
      "  max_results = 10,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# Fetch the results\n",
      "for result in search.results():\n",
      "    print(\"Title:\", result.title)\n",
      "    print(\"Authors:\", result.authors)\n",
      "    print(\"Abstract:\", result.summary)\n",
      "    print(\"Submitted Date:\", result.published)\n",
      "    print(\"URL:\", result.entry_id)\n",
      "    print(\"Potential Applications in Computer Vision:\", \"TBD\")  # Placeholder for manual analysis\n",
      "    print(\"\\n\")\n",
      "```\n",
      "\n",
      "This script will print out the title, authors, abstract, submission date, and URL for up to 10 recent papers related to LLaVA in the field of computer vision. The potential applications in computer vision will need to be determined from the abstract or by reading the paper, as this information is not directly available from the metadata.\n",
      "\n",
      "If you encounter any issues with the script or if you need further assistance, please let me know.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_console_and_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: A Simple LLM Framework for Long-Range Video Question-Answering\n",
      "Authors: [arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Taixi Lu'), arxiv.Result.Author('Md Mohaiminul Islam'), arxiv.Result.Author('Ziyang Wang'), arxiv.Result.Author('Shoubin Yu'), arxiv.Result.Author('Mohit Bansal'), arxiv.Result.Author('Gedas Bertasius')]\n",
      "Abstract: We present LLoVi, a language-based framework for long-range video\n",
      "question-answering (LVQA). Unlike prior long-range video understanding methods,\n",
      "which are often costly and require specialized long-range video modeling design\n",
      "(e.g., memory queues, state-space layers, etc.), our approach uses a\n",
      "frame/clip-level visual captioner (e.g., BLIP2, LaViLa, LLaVA) coupled with a\n",
      "Large Language Model (GPT-3.5, GPT-4) leading to a simple yet surprisingly\n",
      "effective LVQA framework. Specifically, we decompose short and long-range\n",
      "modeling aspects of LVQA into two stages. First, we use a short-term visual\n",
      "captioner to generate textual descriptions of short video clips (0.5-8s in\n",
      "length) densely sampled from a long input video. Afterward, an LLM aggregates\n",
      "the densely extracted short-term captions to perform long-range temporal\n",
      "reasoning needed to understand the whole video and answer a question. To\n",
      "analyze what makes our simple framework so effective, we thoroughly evaluate\n",
      "various components of our system. Our empirical analysis reveals that the\n",
      "choice of the visual captioner and LLM is critical for good LVQA performance.\n",
      "Furthermore, we show that a specialized prompt that asks the LLM first to\n",
      "summarize the noisy short-term visual captions and then answer a given input\n",
      "question leads to a significant LVQA performance boost. On EgoSchema, which is\n",
      "best known as a very long-form video question-answering benchmark, our method\n",
      "achieves 50.3% accuracy, outperforming the previous best-performing approach by\n",
      "18.1% (absolute gain). In addition, our approach outperforms the previous\n",
      "state-of-the-art by 4.1% and 3.1% on NeXT-QA and IntentQA. We also extend LLoVi\n",
      "to grounded LVQA and show that it outperforms all prior methods on the NeXT-GQA\n",
      "dataset. We will release our code at https://github.com/CeeZh/LLoVi.\n",
      "Submitted Date: 2023-12-28 18:58:01+00:00\n",
      "URL: http://arxiv.org/abs/2312.17235v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones\n",
      "Authors: [arxiv.Result.Author('Zhengqing Yuan'), arxiv.Result.Author('Zhaoxu Li'), arxiv.Result.Author('Lichao Sun')]\n",
      "Abstract: In the era of advanced multimodel learning, multimodal large language models\n",
      "(MLLMs) such as GPT-4V have made remarkable strides towards bridging language\n",
      "and visual elements. However, the closed-source nature and considerable\n",
      "computational demand present notable challenges for universal usage and\n",
      "modifications. This is where open-source MLLMs like LLaVA and MiniGPT-4 come\n",
      "in, presenting groundbreaking achievements across tasks. Despite these\n",
      "accomplishments, computational efficiency remains an unresolved issue, as these\n",
      "models, like LLaVA-v1.5-13B, require substantial resources. Addressing these\n",
      "issues, we introduce TinyGPT-V, a new-wave model marrying impressive\n",
      "performance with commonplace computational capacity. It stands out by requiring\n",
      "merely a 24G GPU for training and an 8G GPU or CPU for inference. Built upon\n",
      "Phi-2, TinyGPT-V couples an effective language backbone with pre-trained vision\n",
      "modules from BLIP-2 or CLIP. TinyGPT-V's 2.8B parameters can undergo a unique\n",
      "quantisation process, suitable for local deployment and inference tasks on 8G\n",
      "various devices. Our work fosters further developments for designing\n",
      "cost-effective, efficient, and high-performing MLLMs, expanding their\n",
      "applicability in a broad array of real-world scenarios. Furthermore this paper\n",
      "proposed a new paradigm of Multimodal Large Language Model via small backbones.\n",
      "Our code and training weights are placed at:\n",
      "https://github.com/DLYuanGod/TinyGPT-V and\n",
      "https://huggingface.co/Tyrannosaurus/TinyGPT-V respectively.\n",
      "Submitted Date: 2023-12-28 07:11:41+00:00\n",
      "URL: http://arxiv.org/abs/2312.16862v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: Exploring Multimodal Large Language Models for Radiology Report Error-checking\n",
      "Authors: [arxiv.Result.Author('Jinge Wu'), arxiv.Result.Author('Yunsoo Kim'), arxiv.Result.Author('Eva C. Keller'), arxiv.Result.Author('Jamie Chow'), arxiv.Result.Author('Adam P. Levine'), arxiv.Result.Author('Nikolas Pontikos'), arxiv.Result.Author('Zina Ibrahim'), arxiv.Result.Author('Paul Taylor'), arxiv.Result.Author('Michelle C. Williams'), arxiv.Result.Author('Honghan Wu')]\n",
      "Abstract: This paper proposes one of the first clinical applications of multimodal\n",
      "large language models (LLMs) as an assistant for radiologists to check errors\n",
      "in their reports. We created an evaluation dataset from two real-world\n",
      "radiology datasets (MIMIC-CXR and IU-Xray), with 1,000 subsampled reports each.\n",
      "A subset of original reports was modified to contain synthetic errors by\n",
      "introducing various type of mistakes. The evaluation contained two difficulty\n",
      "levels: SIMPLE for binary error-checking and COMPLEX for identifying error\n",
      "types. LLaVA (Large Language and Visual Assistant) variant models, including\n",
      "our instruction-tuned model, were used for the evaluation. Additionally, a\n",
      "domain expert evaluation was conducted on a small test set. At the SIMPLE\n",
      "level, the LLaVA v1.5 model outperformed other publicly available models.\n",
      "Instruction tuning significantly enhanced performance by 47.4% and 25.4% on\n",
      "MIMIC-CXR and IU-Xray data, respectively. The model also surpassed the domain\n",
      "experts accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets\n",
      "(N=21) of the test set where a clinician did not achieve the correct\n",
      "conclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases.\n",
      "This study marks a promising step toward utilizing multi-modal LLMs to enhance\n",
      "diagnostic accuracy in radiology. The ensemble model demonstrated comparable\n",
      "performance to clinicians, even capturing errors overlooked by humans.\n",
      "Nevertheless, future work is needed to improve the model ability to identify\n",
      "the types of inconsistency.\n",
      "Submitted Date: 2023-12-20 15:20:33+00:00\n",
      "URL: http://arxiv.org/abs/2312.13103v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: VQA4CIR: Boosting Composed Image Retrieval with Visual Question Answering\n",
      "Authors: [arxiv.Result.Author('Chun-Mei Feng'), arxiv.Result.Author('Yang Bai'), arxiv.Result.Author('Tao Luo'), arxiv.Result.Author('Zhen Li'), arxiv.Result.Author('Salman Khan'), arxiv.Result.Author('Wangmeng Zuo'), arxiv.Result.Author('Xinxing Xu'), arxiv.Result.Author('Rick Siow Mong Goh'), arxiv.Result.Author('Yong Liu')]\n",
      "Abstract: Albeit progress has been made in Composed Image Retrieval (CIR), we\n",
      "empirically find that a certain percentage of failure retrieval results are not\n",
      "consistent with their relative captions. To address this issue, this work\n",
      "provides a Visual Question Answering (VQA) perspective to boost the performance\n",
      "of CIR. The resulting VQA4CIR is a post-processing approach and can be directly\n",
      "plugged into existing CIR methods. Given the top-C retrieved images by a CIR\n",
      "method, VQA4CIR aims to decrease the adverse effect of the failure retrieval\n",
      "results being inconsistent with the relative caption. To find the retrieved\n",
      "images inconsistent with the relative caption, we resort to the \"QA generation\n",
      "to VQA\" self-verification pipeline. For QA generation, we suggest fine-tuning\n",
      "LLM (e.g., LLaMA) to generate several pairs of questions and answers from each\n",
      "relative caption. We then fine-tune LVLM (e.g., LLaVA) to obtain the VQA model.\n",
      "By feeding the retrieved image and question to the VQA model, one can find the\n",
      "images inconsistent with relative caption when the answer by VQA is\n",
      "inconsistent with the answer in the QA pair. Consequently, the CIR performance\n",
      "can be boosted by modifying the ranks of inconsistently retrieved images.\n",
      "Experimental results show that our proposed method outperforms state-of-the-art\n",
      "CIR methods on the CIRR and Fashion-IQ datasets.\n",
      "Submitted Date: 2023-12-19 15:56:08+00:00\n",
      "URL: http://arxiv.org/abs/2312.12273v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation\n",
      "Authors: [arxiv.Result.Author('Zhongyi Han'), arxiv.Result.Author('Guanglin Zhou'), arxiv.Result.Author('Rundong He'), arxiv.Result.Author('Jindong Wang'), arxiv.Result.Author('Tailin Wu'), arxiv.Result.Author('Yilong Yin'), arxiv.Result.Author('Salman Khan'), arxiv.Result.Author('Lina Yao'), arxiv.Result.Author('Tongliang Liu'), arxiv.Result.Author('Kun Zhang')]\n",
      "Abstract: In machine learning, generalization against distribution shifts -- where\n",
      "deployment conditions diverge from the training scenarios -- is crucial,\n",
      "particularly in fields like climate modeling, biomedicine, and autonomous\n",
      "driving. The emergence of foundation models, distinguished by their extensive\n",
      "pretraining and task versatility, has led to an increased interest in their\n",
      "adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced\n",
      "publicly accessible multimodal foundation model, with extensive applications\n",
      "across various domains, including anomaly detection, video understanding, image\n",
      "generation, and medical diagnosis. However, its robustness against data\n",
      "distributions remains largely underexplored. Addressing this gap, this study\n",
      "rigorously evaluates GPT-4V's adaptability and generalization capabilities in\n",
      "dynamic environments, benchmarking against prominent models like CLIP and\n",
      "LLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse\n",
      "datasets spanning natural, medical, and molecular domains. We further\n",
      "investigate its adaptability to controlled data perturbations and examine the\n",
      "efficacy of in-context learning as a tool to enhance its adaptation. Our\n",
      "findings delineate GPT-4V's capability boundaries in distribution shifts,\n",
      "shedding light on its strengths and limitations across various scenarios.\n",
      "Importantly, this investigation contributes to our understanding of how AI\n",
      "foundation models generalize to distribution shifts, offering pivotal insights\n",
      "into their adaptability and robustness. Code is publicly available at\n",
      "https://github.com/jameszhou-gl/gpt-4v-distribution-shift.\n",
      "Submitted Date: 2023-12-12 16:48:07+00:00\n",
      "URL: http://arxiv.org/abs/2312.07424v2\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: Honeybee: Locality-enhanced Projector for Multimodal LLM\n",
      "Authors: [arxiv.Result.Author('Junbum Cha'), arxiv.Result.Author('Wooyoung Kang'), arxiv.Result.Author('Jonghwan Mun'), arxiv.Result.Author('Byungseok Roh')]\n",
      "Abstract: In Multimodal Large Language Models (MLLMs), a visual projector plays a\n",
      "crucial role in bridging pre-trained vision encoders with LLMs, enabling\n",
      "profound visual understanding while harnessing the LLMs' robust capabilities.\n",
      "Despite the importance of the visual projector, it has been relatively less\n",
      "explored. In this study, we first identify two essential projector properties:\n",
      "(i) flexibility in managing the number of visual tokens, crucial for MLLMs'\n",
      "overall efficiency, and (ii) preservation of local context from visual\n",
      "features, vital for spatial understanding. Based on these findings, we propose\n",
      "a novel projector design that is both flexible and locality-enhanced,\n",
      "effectively satisfying the two desirable properties. Additionally, we present\n",
      "comprehensive strategies to effectively utilize multiple and multifaceted\n",
      "instruction datasets. Through extensive experiments, we examine the impact of\n",
      "individual design choices. Finally, our proposed MLLM, Honeybee, remarkably\n",
      "outperforms previous state-of-the-art methods across various benchmarks,\n",
      "including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly\n",
      "higher efficiency. Code and models are available at\n",
      "https://github.com/kakaobrain/honeybee.\n",
      "Submitted Date: 2023-12-11 18:59:06+00:00\n",
      "URL: http://arxiv.org/abs/2312.06742v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models\n",
      "Authors: [arxiv.Result.Author('Haoran Wei'), arxiv.Result.Author('Lingyu Kong'), arxiv.Result.Author('Jinyue Chen'), arxiv.Result.Author('Liang Zhao'), arxiv.Result.Author('Zheng Ge'), arxiv.Result.Author('Jinrong Yang'), arxiv.Result.Author('Jianjian Sun'), arxiv.Result.Author('Chunrui Han'), arxiv.Result.Author('Xiangyu Zhang')]\n",
      "Abstract: Modern Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary\n",
      "-- CLIP, which can cover most common vision tasks. However, for some special\n",
      "vision task that needs dense and fine-grained vision perception, e.g.,\n",
      "document-level OCR or chart understanding, especially in non-English scenarios,\n",
      "the CLIP-style vocabulary may encounter low efficiency in tokenizing the vision\n",
      "knowledge and even suffer out-of-vocabulary problem. Accordingly, we propose\n",
      "Vary, an efficient and effective method to scale up the vision vocabulary of\n",
      "LVLMs. The procedures of Vary are naturally divided into two folds: the\n",
      "generation and integration of a new vision vocabulary. In the first phase, we\n",
      "devise a vocabulary network along with a tiny decoder-only transformer to\n",
      "produce the desired vocabulary via autoregression. In the next, we scale up the\n",
      "vanilla vision vocabulary by merging the new one with the original one (CLIP),\n",
      "enabling the LVLMs can quickly garner new features. Compared to the popular\n",
      "BLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while\n",
      "enjoying more excellent fine-grained perception and understanding ability.\n",
      "Specifically, Vary is competent in new document parsing features (OCR or\n",
      "markdown conversion) while achieving 78.2% ANLS in DocVQA and 36.2% in MMVet.\n",
      "Our code will be publicly available on the homepage.\n",
      "Submitted Date: 2023-12-11 04:26:17+00:00\n",
      "URL: http://arxiv.org/abs/2312.06109v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos\n",
      "Authors: [arxiv.Result.Author('Mehmet Saygin Seyfioglu'), arxiv.Result.Author('Wisdom O. Ikezogwo'), arxiv.Result.Author('Fatemeh Ghezloo'), arxiv.Result.Author('Ranjay Krishna'), arxiv.Result.Author('Linda Shapiro')]\n",
      "Abstract: The gigapixel scale of whole slide images (WSIs) poses a challenge for\n",
      "histopathology multi-modal chatbots, requiring a global WSI analysis for\n",
      "diagnosis, compounding evidence from different WSI patches. Current visual\n",
      "instruction datasets, generated through large language models, focus on\n",
      "creating question/answer pairs for individual image patches, which may lack\n",
      "diagnostic capacity on their own in histopathology, further complicated by the\n",
      "absence of spatial grounding in histopathology image captions. To bridge this\n",
      "gap, we introduce Quilt-Instruct, a large-scale dataset of 107,131\n",
      "histopathology-specific instruction question/answer pairs, that is collected by\n",
      "leveraging educational histopathology videos from YouTube, which provides\n",
      "spatial localization of captions by automatically extracting narrators' cursor\n",
      "movements. In addition, we provide contextual reasoning by extracting diagnosis\n",
      "and supporting facts from the entire video content to guide the extrapolative\n",
      "reasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which can\n",
      "reason beyond the given single image patch, enabling diagnostic reasoning and\n",
      "the capability of spatial awareness. To evaluate Quilt-LLaVA, we propose a\n",
      "comprehensive evaluation dataset created from 985 images and 1283\n",
      "human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using\n",
      "public histopathology datasets, where Quilt-LLaVA significantly outperforms\n",
      "SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set\n",
      "VQA. Our code, data, and model are publicly available at quilt-llava.github.io.\n",
      "Submitted Date: 2023-12-07 23:16:37+00:00\n",
      "URL: http://arxiv.org/abs/2312.04746v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: Prompt Highlighter: Interactive Control for Multi-Modal LLMs\n",
      "Authors: [arxiv.Result.Author('Yuechen Zhang'), arxiv.Result.Author('Shengju Qian'), arxiv.Result.Author('Bohao Peng'), arxiv.Result.Author('Shu Liu'), arxiv.Result.Author('Jiaya Jia')]\n",
      "Abstract: This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\n",
      "inference: explicit controllable text generation. Multi-modal LLMs empower\n",
      "multi-modality understanding with the capability of semantic generation yet\n",
      "bring less explainability and heavier reliance on prompt contents due to their\n",
      "autoregressive generative nature. While manipulating prompt formats could\n",
      "improve outputs, designing specific and precise prompts per task can be\n",
      "challenging and ineffective. To tackle this issue, we introduce a novel\n",
      "inference method, Prompt Highlighter, which enables users to highlight specific\n",
      "prompt spans to interactively control the focus during generation. Motivated by\n",
      "the classifier-free diffusion guidance, we form regular and unconditional\n",
      "context pairs based on highlighted tokens, demonstrating that the\n",
      "autoregressive generation in models can be guided in a classifier-free way.\n",
      "Notably, we find that, during inference, guiding the models with highlighted\n",
      "tokens through the attention weights leads to more desired outputs. Our\n",
      "approach is compatible with current LLMs and VLMs, achieving impressive\n",
      "customized generation results without training. Experiments confirm its\n",
      "effectiveness in focusing on input contexts and generating reliable content.\n",
      "Without tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and\n",
      "1552.5 in MME-perception. The code is available at:\n",
      "https://github.com/dvlab-research/Prompt-Highlighter/\n",
      "Submitted Date: 2023-12-07 13:53:29+00:00\n",
      "URL: http://arxiv.org/abs/2312.04302v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "Title: LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models\n",
      "Authors: [arxiv.Result.Author('Hao Zhang'), arxiv.Result.Author('Hongyang Li'), arxiv.Result.Author('Feng Li'), arxiv.Result.Author('Tianhe Ren'), arxiv.Result.Author('Xueyan Zou'), arxiv.Result.Author('Shilong Liu'), arxiv.Result.Author('Shijia Huang'), arxiv.Result.Author('Jianfeng Gao'), arxiv.Result.Author('Lei Zhang'), arxiv.Result.Author('Chunyuan Li'), arxiv.Result.Author('Jianwei Yang')]\n",
      "Abstract: With the recent significant advancements in large multi-modal models (LMMs),\n",
      "the importance of their grounding capability in visual chat is increasingly\n",
      "recognized. Despite recent efforts to enable LMMs to support grounding, their\n",
      "capabilities for grounding and chat are usually separate, and their chat\n",
      "performance drops dramatically when asked to ground. The problem is the lack of\n",
      "a dataset for grounded visual chat (GVC). Existing grounding datasets only\n",
      "contain short captions. To address this issue, we have created GVC data that\n",
      "allows for the combination of grounding and chat capabilities. To better\n",
      "evaluate the GVC capabilities, we have introduced a benchmark called\n",
      "Grounding-Bench. Additionally, we have proposed a model design that can support\n",
      "GVC and various types of visual prompts by connecting segmentation models with\n",
      "language models. Experimental results demonstrate that our model outperforms\n",
      "other LMMs on Grounding-Bench. Furthermore, our model achieves competitive\n",
      "performance on classic grounding benchmarks like RefCOCO/+/g and Flickr30K\n",
      "Entities. Our code will be released at\n",
      "https://github.com/UX-Decoder/LLaVA-Grounding .\n",
      "Submitted Date: 2023-12-05 18:29:31+00:00\n",
      "URL: http://arxiv.org/abs/2312.02949v1\n",
      "Potential Applications in Computer Vision: TBD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mML_Paper_Summarization_Specialist\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the recent papers extracted from arXiv, here are the potential applications in computer vision for the LLaVA framework and related technologies:\n",
      "\n",
      "1. **Long-Range Video Question-Answering (LVQA)**: The LLoVi framework uses a visual captioner coupled with a Large Language Model to perform long-range temporal reasoning for understanding videos and answering questions. This can be applied to video understanding tasks such as video summarization and event detection.\n",
      "\n",
      "2. **Efficient Multimodal Large Language Models**: TinyGPT-V demonstrates the potential for efficient and cost-effective multimodal large language models that can be used for various computer vision tasks on devices with limited computational resources.\n",
      "\n",
      "3. **Radiology Report Error-checking**: LLaVA variant models are used to assist radiologists in checking errors in their reports, which can be applied to medical imaging and diagnostic accuracy enhancement.\n",
      "\n",
      "4. **Composed Image Retrieval (CIR)**: The VQA4CIR method uses a \"QA generation to VQA\" self-verification pipeline to improve the performance of CIR by identifying images inconsistent with their relative captions.\n",
      "\n",
      "5. **Adaptation to Distribution Shifts**: GPT-4V's adaptability and generalization capabilities in dynamic environments can be applied to anomaly detection, medical diagnosis, and other areas where robustness against data distribution shifts is crucial.\n",
      "\n",
      "6. **Locality-enhanced Projector for Multimodal LLMs**: The Honeybee model's projector design can be applied to tasks requiring spatial understanding and is efficient in managing the number of visual tokens.\n",
      "\n",
      "7. **Scaling up Vision Vocabulary for LVLMs**: Vary can be used for document parsing features such as OCR or markdown conversion, especially in non-English scenarios, and can maintain capabilities while providing fine-grained perception and understanding.\n",
      "\n",
      "8. **Visual Instruction Tuning for Histopathology**: Quilt-LLaVA can be applied to diagnostic reasoning in histopathology by enabling spatial awareness and reasoning beyond single image patches.\n",
      "\n",
      "9. **Interactive Control for Multi-Modal LLMs**: Prompt Highlighter allows users to interactively control the focus during generation, which can be applied to customized content generation in various computer vision tasks.\n",
      "\n",
      "10. **Grounded Visual Chat with Large Multimodal Models**: LLaVA-Grounding demonstrates the potential for combining grounding and chat capabilities in visual chat applications, which can be applied to interactive systems that require visual understanding and dialogue.\n",
      "\n",
      "These applications demonstrate the versatility of LLaVA and related technologies in enhancing computer vision tasks, from medical imaging to interactive systems and efficient model deployment on resource-constrained devices.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mML_Paper_Summarization_Specialist\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(config_file_or_env=config_file_or_env)\n",
    "agent_list, agent_configs = new_builder.load(\n",
    "    \"./save_config_c52224ebd16a2e60b348f3f04ac15e79.json\"\n",
    ")  # load previous agent configs\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about LLaVA on arxiv and find its potential applications in computer vision.\",\n",
    "    agent_list=agent_list,\n",
    ")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0cf8f09eef5cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use OpenAI Assistant\n",
    "\n",
    "[The Assistants API](https://platform.openai.com/docs/assistants/overview) allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.\n",
    "AutoBuild also support assistant api by adding `use_oai_assistant=True` to `build()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4051c25b2cd1918c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T10:42:16.740401Z",
     "start_time": "2024-01-01T10:40:37.039210300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Generating agents...\n",
      "['ArXiv_CS_Medical_Paper_Finder_Developer', 'Computational_Biology_Research_Analyst', 'Computer_Science_Literature_Review_Specialist', 'Machine_Learning_Model_Architect', 'Data_Extraction_Scripting_Engineer'] are generated.\n",
      "==> Generating system message...\n",
      "Preparing system message for ArXiv_CS_Medical_Paper_Finder_Developer\n",
      "Preparing system message for Computational_Biology_Research_Analyst\n",
      "Preparing system message for Computer_Science_Literature_Review_Specialist\n",
      "Preparing system message for Machine_Learning_Model_Architect\n",
      "Preparing system message for Data_Extraction_Scripting_Engineer\n",
      "==> Generating description...\n",
      "Preparing description for ArXiv_CS_Medical_Paper_Finder_Developer\n",
      "Preparing description for Computational_Biology_Research_Analyst\n",
      "Preparing description for Computer_Science_Literature_Review_Specialist\n",
      "Preparing description for Machine_Learning_Model_Architect\n",
      "Preparing description for Data_Extraction_Scripting_Engineer\n",
      "==> Creating agents...\n",
      "Creating agent ArXiv_CS_Medical_Paper_Finder_Developer with backbone gpt-4-1106-preview...\n",
      "Creating agent Computational_Biology_Research_Analyst with backbone gpt-4-1106-preview...\n",
      "Creating agent Computer_Science_Literature_Review_Specialist with backbone gpt-4-1106-preview...\n",
      "Creating agent Machine_Learning_Model_Architect with backbone gpt-4-1106-preview...\n",
      "Creating agent Data_Extraction_Scripting_Engineer with backbone gpt-4-1106-preview...\n",
      "Adding user console proxy...\n",
      "\u001b[33mUser_console_and_code_interpreter\u001b[0m (to chat_manager):\n",
      "Find a recent paper about explainable AI on arxiv and find its potential applications in medical.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mArXiv_CS_Medical_Paper_Finder_Developer\u001b[0m (to chat_manager):\n",
      "\n",
      "To perform this task, we will first write a Python script to fetch papers related to \"explainable AI\" from arXiv that are also relevant to the medical field. We will use the `arxiv` library, which is a Python wrapper for the arXiv API. If you don't have the `arxiv` library installed, you can install it using the following command:\n",
      "\n",
      "```bash\n",
      "pip install arxiv\n",
      "```\n",
      "\n",
      "Once we have identified the papers, we will extract potential applications in the medical field from the abstract or conclusion sections if available.\n",
      "\n",
      "Here's the script to find a recent paper about explainable AI from arXiv with relevance to the medical field:\n",
      "\n",
      "```python\n",
      "# Filename: arxiv_explainable_ai_medical.py\n",
      "\n",
      "import arxiv\n",
      "\n",
      "# Query for papers related to \"explainable AI\" in the field of CS and Medical\n",
      "query = 'cat:cs.* AND cat:q-bio.* AND all:explainable AI'\n",
      "sort_by = arxiv.SortCriterion.SubmittedDate\n",
      "sort_order = arxiv.SortOrder.Descending\n",
      "\n",
      "# Perform search query on arXiv\n",
      "search = arxiv.Search(\n",
      "    query=query,\n",
      "    max_results=1,\n",
      "    sort_by=sort_by,\n",
      "    sort_order=sort_order\n",
      ")\n",
      "\n",
      "# Fetch the papers\n",
      "papers = [paper for paper in search.get()]\n",
      "\n",
      "# If there are papers found, print the most recent one's title, authors, and summary\n",
      "if papers:\n",
      "    paper = papers[0]\n",
      "    print(f\"Title: {paper.title}\\n\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in paper.authors)}\\n\")\n",
      "    print(f\"Abstract: {paper.summary}\\n\")\n",
      "    print(f\"Published: {paper.published}\\n\")\n",
      "    print(f\"arXiv ID: {paper.get_short_id()}\\n\")\n",
      "    print(f\"URL: {paper.entry_id}\\n\")\n",
      "else:\n",
      "    print(\"No recent papers on 'explainable AI' found in the medical field on arXiv.\")\n",
      "```\n",
      "\n",
      "To run the script, save it in a Python file and execute it in your Python environment. The script fetches the latest paper based on the defined query and prints out the title, authors, abstract, publication date, arXiv ID, and URL for further reading.\n",
      "\n",
      "Keep in mind that the potential applications in medical would generally be discussed within the paper's text. To extract those, we would typically need to read through the full text, which might involve additional processing steps that are beyond the scope of a simple API query. If the information is not readily available in the abstract, you'll have to review the full text of the paper manually for detailed potential applications in the medical field.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is bash)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_console_and_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Requirement already satisfied: arxiv in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/elpis_ubuntu/miniconda3/envs/llm/lib/python3.11/site-packages (from requests==2.31.0->arxiv) (2023.11.17)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 19, in <module>\n",
      "    papers = [paper for paper in search.get()]\n",
      "                                 ^^^^^^^^^^\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mArXiv_CS_Medical_Paper_Finder_Developer\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems the script encountered an error because the `Search` object from the `arxiv` library does not have a `get` attribute, which is inconsistent with the usual usage of this library. Let's correct that.\n",
      "\n",
      "Instead of using `search.get()`, we will iterate directly over the search object. Here is the corrected script:\n",
      "\n",
      "```python\n",
      "# Filename: arxiv_explainable_ai_medical.py\n",
      "\n",
      "import arxiv\n",
      "import datetime\n",
      "\n",
      "# Query for papers related to \"explainable AI\" in the field of CS and Medical\n",
      "query = 'cat:cs.AI AND all:\"explainable AI\" AND (abs:medical OR abs:\"health care\" OR abs:clinical)'\n",
      "sort_by = arxiv.SortCriterion.SubmittedDate\n",
      "sort_order = arxiv.SortOrder.Descending\n",
      "\n",
      "# Prepare search\n",
      "search = arxiv.Search(\n",
      "    query=query,\n",
      "    max_results=10,\n",
      "    sort_by=sort_by,\n",
      "    sort_order=sort_order,\n",
      ")\n",
      "\n",
      "# Fetch the papers\n",
      "papers = list(search.results())\n",
      "\n",
      "# If there are papers found, print the most recent one's title, authors, and summary\n",
      "if papers:\n",
      "    most_recent_paper = max(papers, key=lambda paper: paper.published)\n",
      "    print(f\"Title: {most_recent_paper.title}\\n\")\n",
      "    print(f\"Authors: {', '.join(author.name for author in most_recent_paper.authors)}\\n\")\n",
      "    print(f\"Abstract: {most_recent_paper.summary}\\n\")\n",
      "    print(f\"Published: {most_recent_paper.published}\\n\")\n",
      "    print(f\"arXiv ID: {most_recent_paper.get_short_id()}\\n\")\n",
      "    print(f\"URL: {most_recent_paper.entry_id}\\n\")\n",
      "else:\n",
      "    print(\"No recent papers on 'explainable AI' found in the medical field on arXiv.\")\n",
      "```\n",
      "\n",
      "Run the above script, and it should now correctly fetch and print the details of the most recent paper related to explainable AI that has potential applications in the medical field. This fix should address the AttributeError by correctly utilizing the `results()` method provided by the `arxiv` library to obtain search results.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_console_and_code_interpreter\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: XAI for In-hospital Mortality Prediction via Multimodal ICU Data\n",
      "\n",
      "Authors: Xingqiao Li, Jindong Gu, Zhiyong Wang, Yancheng Yuan, Bo Du, Fengxiang He\n",
      "\n",
      "Abstract: Predicting in-hospital mortality for intensive care unit (ICU) patients is\n",
      "key to final clinical outcomes. AI has shown advantaged accuracy but suffers\n",
      "from the lack of explainability. To address this issue, this paper proposes an\n",
      "eXplainable Multimodal Mortality Predictor (X-MMP) approaching an efficient,\n",
      "explainable AI solution for predicting in-hospital mortality via multimodal ICU\n",
      "data. We employ multimodal learning in our framework, which can receive\n",
      "heterogeneous inputs from clinical data and make decisions. Furthermore, we\n",
      "introduce an explainable method, namely Layer-Wise Propagation to Transformer,\n",
      "as a proper extension of the LRP method to Transformers, producing explanations\n",
      "over multimodal inputs and revealing the salient features attributed to\n",
      "prediction. Moreover, the contribution of each modality to clinical outcomes\n",
      "can be visualized, assisting clinicians in understanding the reasoning behind\n",
      "decision-making. We construct a multimodal dataset based on MIMIC-III and\n",
      "MIMIC-III Waveform Database Matched Subset. Comprehensive experiments on\n",
      "benchmark datasets demonstrate that our proposed framework can achieve\n",
      "reasonable interpretation with competitive prediction accuracy. In particular,\n",
      "our framework can be easily transferred to other clinical tasks, which\n",
      "facilitates the discovery of crucial factors in healthcare research.\n",
      "\n",
      "Published: 2023-12-29 14:28:04+00:00\n",
      "\n",
      "arXiv ID: 2312.17624v1\n",
      "\n",
      "URL: http://arxiv.org/abs/2312.17624v1\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mComputational_Biology_Research_Analyst\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the output, we have identified a recent paper from arXiv:\n",
      "\n",
      "Title: **\"XAI for In-hospital Mortality Prediction via Multimodal ICU Data\"**\n",
      "\n",
      "Authors: **Xingqiao Li, Jindong Gu, Zhiyong Wang, Yancheng Yuan, Bo Du, Fengxiang He**\n",
      "\n",
      "Abstract Summary: The paper addresses the challenge of explainability in AI for predicting in-hospital mortality of ICU patients using multimodal data. They propose the eXplainable Multimodal Mortality Predictor (X-MMP), which combines multimodal learning and an explainability method called Layer-Wise Propagation to Transformer (LWP-T). This allows the model to make decisions with explanations across multimodal inputs, identifying important features for the prediction and visualizing the contribution of each modality to the clinical outcomes. This could assist clinicians in comprehending the AI's decision-making process.\n",
      "\n",
      "Published: **December 29, 2023**\n",
      "\n",
      "arXiv ID: **2312.17624v1**\n",
      "\n",
      "URL: [http://arxiv.org/abs/2312.17624v1](http://arxiv.org/abs/2312.17624v1)\n",
      "\n",
      "**Potential Applications in Medical Field**:\n",
      "\n",
      "1. **Improved decision-making in intensive care units (ICUs):** By providing explainability for its predictions regarding patient mortality, clinicians can understand the reasoning behind AI-driven prognoses and make more informed treatment decisions.\n",
      "\n",
      "2. **Enhanced clinician trust in AI technologies:** Explainable outputs can build clinician trust in AI systems, thereby potentially increasing the adoption of AI tools in critical care settings.\n",
      "\n",
      "3. **Identification of crucial health factors:** The framework assists in discovering important factors in healthcare research, possibly leading to new insights into patient care and management.\n",
      "\n",
      "4. **Education and training:** The visualizations and explanations provided by X-MMP could be used in medical education and training, helping healthcare professionals to better understand the factors influencing patient outcomes in the ICU.\n",
      "\n",
      "5. **Transferability to other clinical tasks:** The framework can be adapted to other clinical prediction tasks, making it a versatile tool for various applications within the healthcare domain.\n",
      "\n",
      "6. **Contribution analysis of multimodal data:** Understanding how various types of data (vitals, lab results, waveforms, etc.) influence predictions can lead to better multimodal data integration in clinical workflows.\n",
      "\n",
      "This paper showcases how explainable AI can directly impact healthcare by enhancing the transparency and interpretability of AI models, ultimately supporting clinical decision-making and patient care. The application of such technology could be pivotal in advancing personalized medicine and tailored treatment plans for patients in critical conditions. \n",
      "\n",
      "If this information satisfies the task requirements, please let me know, or if there are further inquiries, feel free to ask.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mComputer_Science_Literature_Review_Specialist\u001b[0m (to chat_manager):\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "All agents have been cleared.\n"
     ]
    }
   ],
   "source": [
    "new_builder = AgentBuilder(\n",
    "    config_file_or_env=config_file_or_env, builder_model=\"gpt-4-1106-preview\", agent_model=\"gpt-4-1106-preview\"\n",
    ")\n",
    "agent_list, agent_configs = new_builder.build(\n",
    "    building_task, llm_config, use_oai_assistant=True\n",
    ")  # Transfer to OpenAI assistant API.\n",
    "start_task(\n",
    "    execution_task=\"Find a recent paper about explainable AI on arxiv and find its potential applications in medical.\",\n",
    "    agent_list=agent_list,\n",
    ")\n",
    "new_builder.clear_all_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdc75f8810926a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
